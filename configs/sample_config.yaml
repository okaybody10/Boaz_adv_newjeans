modality: "text-image" # "text" | "image" | "text-image" | "text-image-dialogue"
num_classes: 2 # 2 | 3 | 6
batch_size: 64 # (int)
learning_rate: 1.0e-5 # (float) Note that the mantissa must have a decimal point to be parsed by YAML as a float (and not a str)
num_epochs: 20 # (int)
dropout_p: 0.1 # (float)
text_embedder: "snunlp/KR-SBERT-V40K-klueNLI-augSTS" # "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens" # "all-mpnet-base-v2" | "all-distilroberta-v1"
dialogue_summarization_model: null # None=Transformers.Pipeline default i.e. "sshleifer/distilbart-cnn-12-6" | "bart-large-cnn" | "t5-small" | "t5-base" | "t5-large"
train_data_path: "./data/train.csv" # (str)
test_data_path: "./data/test.csv" # (str)
preprocessed_train_dataframe_path: "./data/train__text_image__dataframe.pkl" # (str)
preprocessed_test_dataframe_path: "./data/test__text_image__dataframe.pkl" # (str)
gpus: [0] # [0] | [1] | [0, 1] Note that it must be a list of ints
trained_model_version: 0 # (int)
trained_model_path: "./lightning_logs/version_2/checkpoints" # (str)